rbac:
  # TODO still needs work to fully lock down scope etc
  # See issue #88
  create: false

api:

  ## Deployment component name
  name: server
  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  replicaCount: 1

  ## api Deployment Strategy type
  strategy:
    type: RollingUpdate
  #   type: Recreate

  ## Annotations to be added to api pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: linkage

  # Settings for the nginx proxy
  www:

    image:
      repository: data61/anonlink-nginx
      tag: "v1.4.3"
      ## "IfNotPresent" or "Always"
      pullPolicy: IfNotPresent

    ## Nginx proxy server resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 200m
        memory: 256Mi

  app:
    image:
      repository: data61/anonlink-app
      tag: "v1.11.1"
      ## Options: "IfNotPresent" or "Always"
      pullPolicy: IfNotPresent


    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      limits:
        cpu: 1
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 512Mi

    debug: "false"

  dbinit:
    enabled: "true"

    ## Database init can only run during chart install
    ## It cannot be updated! So we have a separate image + tag
    image:
      repository: data61/anonlink-app
      tag: "v1.11.2"

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi

  ingress:
    enabled: true

    ## Ingress annotations
    ##
    annotations: # {}
     kubernetes.io/ingress.class: nginx

     ## To handle large uploads we increase the proxy buffer size
     ingress.kubernetes.io/proxy-body-size: 4096m
     nginx.ingress.kubernetes.io/proxy-body-size: 4096m
     ## Redirect to ssl
     nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
     ## Example ingress annotations for certmanager
     certmanager.k8s.io/cluster-issuer: letsencrypt-cluster-issuer
     ingress.kubernetes.io/force-ssl-redirect: "true"

    ## Entity Service API Ingress hostnames
    ## Must be provided if Ingress is enabled
    hosts:
     - beta.anonlink.data61.xyz

    ## Entity Service API Ingress TLS configuration
    ## This example setup is for nginx-ingress. We use certificate manager.
    ## to create the TLS secret in the namespace with the name
    ## below.
    tls: # []
    - secretName: beta-anonlink-data61-tls
      hosts:
      - beta.anonlink.data61.xyz

  service:
    annotations: []
    labels:
      tier: frontend
    clusterIp: ""

    ## Expose the service to be accessed from outside the cluster (LoadBalancer service).
    ## or access it from within the cluster (ClusterIP service).
    ## Set the service type and the port to serve it.
    ## ref: http://kubernetes.io/docs/user-guide/services/
    ## Most likely ingress is enabled so this should be ClusterIP,
    ## Otherwise "LoadBalancer".
    type: ClusterIP
    servicePort: 80

    ## If using a load balancer on AWS you can optionally lock down access
    ## to a given IP range. Provide a list of IPs that are allowed via a
    ## security group.
    loadBalancerSourceRanges: []

workers:
  name: "matcher"

  image:
    repository: "data61/anonlink-app"
    tag: "v1.11.2"
    pullPolicy: IfNotPresent

  ## The number of workers for this deployment
  replicaCount: 2

  autoscaler:
    enabled: false

  podAnnotations: {}

  #strategy: ""

  ## Additional Entity Service Worker container arguments
  ##
  extraArgs: {}

  ## Worker configuration
  ## These settings populate the deployment's configmap.

  debug: false

  # Desired task size in "number of comparisons"
  # Note there is some overhead creating a task and a single dedicated cpu core can do between 50M and 100M
  # comparisons per second, so much lower that 100M isn't generally worth splitting across celery workers.
  CHUNK_SIZE_AIM: "300_000_000"

  # More than this many entities and we skip caching in redis
  MAX_CACHE_SIZE: "1_000_000"

  # Optional path to a logging configuration file
  # Eventually this config can be stored in a configmap
  # See https://github.com/data61/anonlink-entity-service/issues/323
  #LOG_CFG: "entityservice/verbose_logging.yaml"

  celery:
    PREFETCH_MULTIPLIER: "1"
    MAX_TASKS_PER_CHILD: "4096"
    ACKS_LATE: "true"

  monitor:
    enabled: false

  ## https://kubernetes.io/docs/user-guide/compute-resources
  resources:
    requests:
      memory: 2Gi
      cpu: 800m
#    limits:
#      memory: 2Gi
#      cpu: 200m

  ## At least one "high memory" worker is also required
  highmemory:
    replicaCount: 1
    resources:
      requests:
        memory: 16Gi
        cpu: 800m

postgresql:
  # See available settings and defaults at:
  # https://github.com/kubernetes/charts/tree/master/stable/postgresql
  nameOverride: "db"

  persistence:
    enabled: true
    size: 8Gi
  metrics:
    enabled: false
  resources:
    #limits:
    #  memory: 8Gi
    requests:
      #memory: 1Gi
      cpu: 100m

global:
  postgresql:
    postgresqlUsername: postgres
    #postgresqlPassword:


redis:
  ## Note the `server` and `use_sentinel` options are ignored if provisioning redis
  ## using this chart.

  ## External redis server url/ip
  server: ""

  ## Does the external redis server support the sentinel protocol?
  use_sentinel: false

  ## Note if deploying redis-ha you MUST have the same password below!
  password: "9X9RFrRnQHcB"

redis-ha:
  ## Settings for configuration of a provisioned redis ha cluster.
  ## https://github.com/helm/charts/tree/master/stable/redis-ha
  ## Provisioning is controlled in the `provision` section
  auth: true
  redisPassword: "9X9RFrRnQHcB"
  replicas: 3
  redis:
    resources:
      requests:
        memory: 512Mi
        cpu: 100m
      limits:
        memory: 10Gi
  sentinel:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 256Mi
  persistentVolume:
    enabled: true
    size: 10Gi
  nameOverride: "memstore"


minio:
  # Configure the object storage
  # https://github.com/helm/charts/blob/master/stable/minio/values.yaml

  ## Access credentials for the object store
  accessKey: "xtoNfbmB7cUaW4sLPzKBXVPXd8oAvVbvkAosxsEXAMPLE"
  secretKey: "6hkE6FzMLBd4tkzhVjXjPdEzQikmEtjRDQwyobEXAMPLE"

  ## Settings required for connecting to another object store, the server is ignored
  ## if provisioning minio during deployment.
  server: "s3.amazonaws.com"

  bucket: "confidentialcomputing-k8s-objectstore"

  ## Settings for deploying standalone object store
  ## https://github.com/kubernetes/charts/tree/master/stable/minio#configuration
  ## Can distribute the object store across multiple nodes.
  mode: "standalone"
  service.type: "ClusterIP"
  persistence:
    enabled: true
    size: 50Gi
    storageClass: "default"
  nameOverride: "minio"


provision:
  # enable to deploy a standalone version of each service as part of the helm deployment
  minio: true
  postgresql: true
  redis: true
