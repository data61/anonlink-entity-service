rbac:
  ## TODO still needs work to fully lock down scope etc
  ## See issue #88
  create: false

api:

  ## Deployment component name
  name: server
  ## Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  replicaCount: 1

  ## api Deployment Strategy type
  strategy:
    type: RollingUpdate
  #   type: Recreate

  ## Annotations to be added to api pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: linkage

  ## Settings for the nginx proxy
  www:

    image:
      repository: data61/anonlink-nginx
      tag: "v1.4.2"
      # pullPolicy: Always
      pullPolicy: IfNotPresent

    ## Nginx proxy server resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 200m
        memory: 256Mi

  app:
    image:
      repository: data61/anonlink-app
      tag: "v1.11.1"
      # pullPolicy: IfNotPresent
      pullPolicy: Always


    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      limits:
        cpu: 1
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 512Mi

    debug: "false"

  dbinit:
    enabled: "true"

    ## Database init can only run during chart install
    ## It cannot be updated! So we have a separate image + tag
    image:
      repository: data61/anonlink-app
      tag: "v1.11.1"

    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi

  ingress:
    ## By default, we do not want the service to be accessible outside of the cluster.
    enabled: false

    ## Ingress annotations
    annotations: # {}
     kubernetes.io/ingress.class: nginx

     ## To handle large uploads we increase the proxy buffer size
     ingress.kubernetes.io/proxy-body-size: 4096m
     nginx.ingress.kubernetes.io/proxy-body-size: 4096m
     ## Redirect to ssl
     nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-read-timeout: "60"
     ## Example ingress annotations for certmanager
     certmanager.k8s.io/cluster-issuer: letsencrypt-cluster-issuer
     ingress.kubernetes.io/force-ssl-redirect: "true"

    ## Entity Service API Ingress hostnames
    ## Must be provided if Ingress is enabled
    hosts:
     - beta.anonlink.data61.xyz

    ## Entity Service API Ingress TLS configuration
    ## This example setup is for nginx-ingress. We use certificate manager.
    ## to create the TLS secret in the namespace with the name
    ## below.
    tls: # []
    ## secretName is the kubernetes secret which will contain the TLS secret and certificates
    ## for the provided host url. It is automatically generated from the deployed certmanager.
    - secretName: beta-anonlink-data61-tls
      hosts:
      - beta.anonlink.data61.xyz

  service:
    annotations: []
    labels:
      tier: frontend
    clusterIp: ""

    ## Expose the service to be accessed from outside the cluster (LoadBalancer service).
    ## or access it from within the cluster (ClusterIP service).
    ## Set the service type and the port to serve it.
    ## Ref: http://kubernetes.io/docs/user-guide/services/
    ## Most likely ingress is enabled so this should be ClusterIP,
    ## Otherwise "LoadBalancer".
    type: ClusterIP
    servicePort: 80

    ## If using a load balancer on AWS you can optionally lock down access
    ## to a given IP range. Provide a list of IPs that are allowed via a
    ## security group.
    loadBalancerSourceRanges: []

  metrics:
    enabled: true

    ## A k8s service exposing application metrics
    service:
      annotations:
        prometheus.io/scrape: 'true'
      type: ClusterIP
      #labels: []

workers:
  name: "matcher"

  image:
    repository: "data61/anonlink-app"
    tag: "v1.11.1"
    pullPolicy: Always

  ## The number of workers for this deployment
  replicaCount: 1

  autoscaler:
    enabled: false

  podAnnotations: {}

  #strategy: ""

  ## Additional Entity Service Worker container arguments
  ##
  extraArgs: {}

  ## Worker configuration
  ## These settings populate the deployment's configmap.

  debug: false

  ## Desired task size in "number of comparisons"
  ## Note there is some overhead creating a task and a single dedicated cpu core can do between 50M and 100M
  ## comparisons per second, so much lower that 100M isn't generally worth splitting across celery workers.
  CHUNK_SIZE_AIM: "300_000_000"

  ## More than this many entities and we skip caching in redis
  MAX_CACHE_SIZE: "1_000_000"

  ## Optional path to a logging configuration file
  ## Eventually this config can be stored in a configmap
  ## See https://github.com/data61/anonlink-entity-service/issues/323
  #LOG_CFG: "entityservice/verbose_logging.yaml"

  ## Specific configuration for celery
  ## Note that these configurations are the same for a "normal" worker, and a "highmemeroy" one,
  ## except for the requested resources and replicaCount which can differ.
  celery:
    ## Number of fork worker celery node will have. It is recommended to use the same concurrency as workers.resources.limits.cpu
    CONCURRENCY: "2"
    ## How many messages to prefetch at a time multiplied by the number of concurrent processes. Set to 1 because
    ## our tasks are usually quite "long".
    PREFETCH_MULTIPLIER: "1"
    ## Maximum number of tasks a pool worker process can execute before itâ€™s replaced with a new one
    ## Low number is recommended, otherwise the celery workers may exhaust the available memory and threads.
    ## Cf issue https://github.com/data61/anonlink-entity-service/issues/410
    MAX_TASKS_PER_CHILD: "30"
    ## Late ack means the task messages will be acknowledged after the task has been executed, not just before.
    ACKS_LATE: "true"

  ## Currently, enable only the monitoring of celery.
  monitor:
    enabled: false

  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      memory: 500Mi
      cpu: 500m
    ## It is recommended to set limits. celery does not like to share resources.
    limits:
      memory: 1Gi
      cpu: 2

  ## At least one "high memory" worker is also required
  highmemory:
    replicaCount: 1
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      requests:
        memory: 2Gi
        cpu: 1
      ## It is recommended to set limits. celery does not like to share resources.
      limits:
        memory: 2Gi
        cpu: 2

postgresql:
  ## See available settings and defaults at:
  ## https://github.com/kubernetes/charts/tree/master/stable/postgresql
  nameOverride: "db"

  persistence:
    enabled: false
    size: 8Gi
  metrics:
    enabled: true
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    #limits:
    #  memory: 8Gi
    requests:
      #memory: 1Gi
      cpu: 100m

global:
  postgresql:
    postgresqlDatabase: postgres
    postgresqlUsername: postgres
    postgresqlPassword: "examplePostgresPassword"


## In this section, we are not installing any redis pods. The main goal is to get configuration values for the
## other services.
redis:
  ## Note the `server` and `use_sentinel` options are ignored if provisioning redis
  ## using this chart.

  ## External redis server url/ip
  server: ""

  ## Does the external redis server support the sentinel protocol?
  use_sentinel: false

  ## Note if deploying redis-ha you MUST have the same password below!
  password: "exampleRedisPassword"


redis-ha:
  ## Settings for configuration of a provisioned redis ha cluster.
  ## https://github.com/helm/charts/tree/master/stable/redis-ha
  ## Provisioning is controlled in the `provision` section
  auth: true
  redisPassword: "exampleRedisPassword"
  replicas: 3
  redis:
    resources:
      requests:
        memory: 512Mi
        cpu: 100m
      limits:
        memory: 10Gi
  sentinel:
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 256Mi
  persistentVolume:
    enabled: false
    size: 10Gi
  nameOverride: "memstore"


minio:
  ## Configure the object storage
  ## https://github.com/helm/charts/blob/master/stable/minio/values.yaml

  ## Access credentials for the object store
  accessKey: "exampleMinioAccessKey"
  secretKey: "exampleMinioSecretKet"

  ## Settings required for connecting to another object store, the server is ignored
  ## if provisioning minio during deployment.
  server: "s3.amazonaws.com"

  bucket: "example-bucket"

  ## Settings for deploying standalone object store
  ## https://github.com/kubernetes/charts/tree/master/stable/minio#configuration
  ## Can distribute the object store across multiple nodes.
  mode: "standalone"
  service.type: "ClusterIP"
  persistence:
    enabled: false
    size: 50Gi
    storageClass: "default"
  nameOverride: "minio"
  
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    requests:
      memory: 256Mi
      cpu: 250m
    limits:
      memory: 5Gi


provision:
  # enable to deploy a standalone version of each service as part of the helm deployment
  minio: true
  postgresql: true
  redis: true
