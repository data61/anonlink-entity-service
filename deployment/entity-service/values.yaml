rbac:
  # TODO still needs work to fully lock down scope etc
  # See https://github.com/n1analytics/entity-service/issues/88
  create: false

api:

  ## Deployment component name
  name: server
  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  replicaCount: 1

  ## api Deployment Strategy type
  strategy:
    type: RollingUpdate
  #   type: Recreate

  ## Annotations to be added to api pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: linkage

  # Settings for the nginx proxy
  www:

    image:
      repository: data61/anonlink-nginx
      tag: "v1.4.2"
      ## "IfNotPresent" or "Always"
      pullPolicy: IfNotPresent

    ## Nginx proxy server resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 200m
        memory: 256Mi

  app:
    image:
      repository: data61/anonlink-app
      tag: "v1.11.0"
      ## Options: "IfNotPresent" or "Always"
      pullPolicy: Always


    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      limits:
        cpu: 1
        memory: 8Gi
      requests:
        cpu: 500m
        memory: 512Mi

    debug: "false"

  dbinit:
    enabled: "true"

    ## Database init can only run during chart install
    ## It cannot be updated! So we have a separate image + tag
    image:
      repository: data61/anonlink-app
      tag: "v1.11.0"

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi

  ingress:
    enabled: true

    ## Ingress annotations
    ##
    annotations: #{}
     kubernetes.io/ingress.class: nginx
     certmanager.k8s.io/cluster-issuer: letsencrypt-cluster-issuer
     ## To handle large uploads we increase the nginx buffer size
     ingress.kubernetes.io/proxy-body-size: 4096m
     ## Redirect to ssl
     ingress.kubernetes.io/force-ssl-redirect: "true"
     nginx.ingress.kubernetes.io/proxy-body-size: 4096m
     nginx.ingress.kubernetes.io/proxy-connect-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-send-timeout: "60"
     nginx.ingress.kubernetes.io/proxy-read-timeout: "60"

    ## Entity Service API Ingress hostnames
    ## Must be provided if Ingress is enabled
    hosts:
     - beta.anonlink.data61.xyz

    ## Entity Service API Ingress TLS configuration
    ## This example setup is for nginx-ingress. We use certificate manager.
    ## to create the TLS secret in the namespace with the name
    ## below.
    tls: #[]
    - secretName: beta-anonlink-data61-tls
      hosts:
      - beta.anonlink.data61.xyz

  service:
    annotations: []
    labels:
      tier: frontend
    clusterIp: ""

    ## Expose the service to be accessed from outside the cluster (LoadBalancer service).
    ## or access it from within the cluster (ClusterIP service).
    ## Set the service type and the port to serve it.
    ## ref: http://kubernetes.io/docs/user-guide/services/
    ## Most likely if ingress is enabled, this should be ClusterIP,
    ## Otherwise LoadBalancer. Note you should manually adjust the timeout on
    ## an AWS ELB to 300+ seconds (from the default of 60).
    type: ClusterIP

    servicePort: 80

    ## If using a load balancer on AWS you can optionally lock down access
    ## to a given IP range. Provide a list of IPs that are allowed via a
    ## security group.
    loadBalancerSourceRanges: []
    ##  - 130.155.157.0/24

workers:
  name: "matcher"

  image:
    repository: "data61/anonlink-app"
    tag: "v1.11.0"
    pullPolicy: Always

  ## Usually this is set to 1 per compute node
  replicaCount: 2
  autoscaler:
    enabled: false
  debug: true

  podAnnotations: {}

  #strategy: ""

  ## Additional Entity Service Worker container arguments
  ##
  extraArgs: {}

  # TODO move these settings into a config file

  SMALL_COMPARISON_CHUNK_SIZE: "100000000"
  LARGE_COMPARISON_CHUNK_SIZE: "1000000000"

  # A minimum of say 100M because any smaller than
  # this isn't worth splitting across nodes.
  SMALL_JOB_SIZE: "100000000"
  LARGE_JOB_SIZE: "100000000000"

  # Default threshold for considering something a match
  MATCH_THRESHOLD: "0.90"

  # More than this many entities and we skip caching in redis
  MAX_CACHE_SIZE: "1000000"

  celery:
    PREFETCH_MULTIPLIER: "1"
    MAX_TASKS_PER_CHILD: "4096"
    ACKS_LATE: "true"

  monitor:
    enabled: false

  ## https://kubernetes.io/docs/user-guide/compute-resources
  resources:
    requests:
      memory: 2Gi
      cpu: 800m
#    limits:
#      memory: 2Gi
#      cpu: 200m

  # At least one "high memory" worker is also required
  highmemory:
    replicaCount: 2
    resources:
      requests:
        memory: 16Gi
        cpu: 800m

postgresql:
  # See available settings and defaults at:
  # https://github.com/kubernetes/charts/tree/master/stable/postgresql
  nameOverride: "db"

  persistence:
    enabled: true
    size: 8Gi
  metrics:
    enabled: false
  resources:
    #limits:
    #  memory: 8Gi
    requests:
      #memory: 1Gi
      cpu: 100m

global:
  postgresql:
    postgresqlUsername: postgres
    #postgresqlPassword:


redis:
  ## Note the `server` and `use_sentinel` options are ignored if provisioning redis
  ## using this chart.

  ## External redis server url/ip
  server: ""

  ## Does the external redis server support the sentinel protocol?
  use_sentinel: false

  ## Note if deploying redis-ha you MUST have the same password below!
  password: "9X9RFrRnQHcB"

redis-ha:
  ## Settings for configuration of a provisioned redis ha cluster.
  ## https://github.com/helm/charts/tree/master/stable/redis-ha
  ## Provisioning is controlled in the `provision` section
  auth: true
  redisPassword: "9X9RFrRnQHcB"
  replicas: 3
  redis:
    resources:
      requests:
        memory: 512Mi
        cpu: 100m
      limits:
        memory: 10Gi
  sentinel:
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 256Mi
  persistentVolume:
    enabled: true
    size: 10Gi
  nameOverride: "memstore"


minio:
  # Configure the object storage
  # https://github.com/helm/charts/blob/master/stable/minio/values.yaml

  ## Access credentials for the object store
  accessKey: "xtoNfbmB7cUaW4sLPzKBXVPXd8oAvVbvkAosxsEXAMPLE"
  secretKey: "6hkE6FzMLBd4tkzhVjXjPdEzQikmEtjRDQwyobEXAMPLE"

  ## Settings required for connecting to another object store, the server is ignored
  ## if provisioning minio during deployment.
  server: "s3.amazonaws.com"

  bucket: "confidentialcomputing-k8s-objectstore"

  ## Settings for deploying standalone object store
  ## https://github.com/kubernetes/charts/tree/master/stable/minio#configuration
  ## Can distribute the object store across multiple nodes.
  mode: "standalone"
  service.type: "ClusterIP"
  persistence:
    enabled: true
    size: 50Gi
    storageClass: "default"
  nameOverride: "minio"


provision:
  # enable to deploy a standalone version of minio, otherwise connect minio to another service (e.g. S3)
  minio: true
  postgresql: true
  redis: true
