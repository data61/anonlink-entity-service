{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Entity Service: Multiparty linkage demo\n",
    "This notebook is a demonstration of the multiparty linkage capability that has been implemented in the Entity Service.\n",
    "\n",
    "We show how five parties may upload their hashed data to the Entity Service to obtain a multiparty linkage result. This result identifies each entity across all datasets in which they are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Check the status of the Entity Service\n",
    "Ensure that it is running and that we have the correct version. Multiparty support was introduced in version 1.11.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_count': 5944, 'rate': 2260983, 'status': 'ok'}\n",
      "{'anonlink': '0.12.5', 'entityservice': 'v1.13.0-alpha', 'python': '3.7.5'}\n"
     ]
    }
   ],
   "source": [
    "SERVER = os.getenv(\"SERVER\", \"https://testing.es.data61.xyz\")\n",
    "PREFIX = f\"{SERVER}/api/v1\"\n",
    "print(requests.get(f\"{PREFIX}/status\").json())\n",
    "print(requests.get(f\"{PREFIX}/version\").json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Create a new project\n",
    "We create a new multiparty project for five parties by specifying the number of parties and the output type (currently only the `group` output type supports multiparty linkage). Retain the `project_id`, so we can find the project later. Also retain the `result_token`, so we can retrieve the results (careful: anyone with this token has access to the results). Finally, the `update_tokens` identify the five data data providers and permit them to upload CLKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_id: 21d8916332764c00c0861f1dda132c633c731c377fd89696\n",
      "\n",
      "result_token: 4b8c53796161aad56414631fd553d5905256ea5cba0476e8\n",
      "\n",
      "update_tokens: ['f3dafb72996cbc0f453f2acde9dd0e037066039d492c96ee', '28c6cb8b3f85bb528574d51c1f67953af7bb9b835b119451', '028b0b1c05b1e669c7b5bf13caf3a53022481d867c3c0fb9', '105c8d242b51f30388f6f8b0bd4d32189127ea760d22377e', '36955c914e3e0d1aed86a5af32027dfb8a8169532ba4125e']\n"
     ]
    }
   ],
   "source": [
    "project_info = requests.post(\n",
    "    f\"{PREFIX}/projects\",\n",
    "    json={\n",
    "        \"schema\": {},\n",
    "        \"result_type\": \"groups\",\n",
    "        \"number_parties\": 5,\n",
    "        \"name\": \"example project\"\n",
    "    }\n",
    ").json()\n",
    "project_id = project_info[\"project_id\"]\n",
    "result_token = project_info[\"result_token\"]\n",
    "update_tokens = project_info[\"update_tokens\"]\n",
    "\n",
    "print(\"project_id:\", project_id)\n",
    "print()\n",
    "print(\"result_token:\", result_token)\n",
    "print()\n",
    "print(\"update_tokens:\", update_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Upload the hashed data\n",
    "This is where each party uploads their CLKs into the service. Here, we do the work of all five data providers inside this for loop. In a deployment scenario, each data provider would be uploading their own CLKs using their own update token.\n",
    "\n",
    "These CLKs are already hashed using [clkhash](https://github.com/data61/clkhash), so for each data provider, we just need to upload their corresponding hash file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data provider 1: {\n",
      "  \"message\": \"Updated\",\n",
      "  \"receipt_token\": \"3e102ce587ae97feb18aebf7596aee5ba3ba5b6a41d5bedf\"\n",
      "}\n",
      "\n",
      "Data provider 2: {\n",
      "  \"message\": \"Updated\",\n",
      "  \"receipt_token\": \"ab758b30126ddc083bf65749773fc5856719b4273adc0703\"\n",
      "}\n",
      "\n",
      "Data provider 3: {\n",
      "  \"message\": \"Updated\",\n",
      "  \"receipt_token\": \"e013c252746cbc5ceb00b4009500769ceb63389de886137c\"\n",
      "}\n",
      "\n",
      "Data provider 4: {\n",
      "  \"message\": \"Updated\",\n",
      "  \"receipt_token\": \"f2f38a3206197dd46b53c4c6da079527552d7c6e24b9b63e\"\n",
      "}\n",
      "\n",
      "Data provider 5: {\n",
      "  \"message\": \"Updated\",\n",
      "  \"receipt_token\": \"e489cf14d65b211dd6c8b98b1a902f04e3b09c0e3da21a44\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(update_tokens, start=1):\n",
    "    with open(f\"data/clks-{i}.json\") as f:\n",
    "        r = requests.post(\n",
    "            f\"{PREFIX}/projects/{project_id}/clks\",\n",
    "            data=f,\n",
    "            headers={\n",
    "                \"Authorization\": token,\n",
    "                \"content-type\": \"application/json\"\n",
    "            }\n",
    "        )\n",
    "    print(f\"Data provider {i}: {r.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Begin a run\n",
    "The data providers have uploaded their CLKs, so we may begin the computation. This computation may be repeated multiple times, each time with different parameters. Each such repetition is called a run. The most important parameter to vary between runs is the similarity threshold. Two records whose similarity is above this threshold will be considered to describe the same entity.\n",
    "\n",
    "Here, we perform one run. We (somewhat arbitrarily) choose the threshold to be 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    f\"{PREFIX}/projects/{project_id}/runs\",\n",
    "    headers={\n",
    "        \"Authorization\": result_token\n",
    "    },\n",
    "    json={\n",
    "        \"threshold\": 0.8\n",
    "    }\n",
    ")\n",
    "run_id = r.json()[\"run_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Check the status\n",
    "Let's see whether the run has finished ('state' is 'completed')!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_stage': {'description': 'compute similarity scores',\n",
       "  'number': 2,\n",
       "  'progress': {'absolute': 31440720,\n",
       "   'description': 'number of already computed similarity scores',\n",
       "   'relative': 0.2984721650891483}},\n",
       " 'stages': 3,\n",
       " 'state': 'running',\n",
       " 'time_added': '2019-11-18T02:52:30.352381+00:00',\n",
       " 'time_started': '2019-11-18T02:52:30.373760+00:00'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"{PREFIX}/projects/{project_id}/runs/{run_id}/status\",\n",
    "    headers={\n",
    "        \"Authorization\": result_token\n",
    "    }\n",
    ")\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after some delay (depending on the size) we can fetch the results. Waiting for completion can be achieved by directly polling the REST API using `requests`, however for simplicity we will just use the `watch_run_status` function provided in `clkhash.rest_client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: completed\n",
      "Stage (3/3): compute output\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from clkhash.rest_client import RestClient\n",
    "from clkhash.rest_client import format_run_status\n",
    "rest_client = RestClient(SERVER)\n",
    "for update in rest_client.watch_run_status(project_id, run_id, result_token, timeout=300):\n",
    "    clear_output(wait=True)\n",
    "    print(format_run_status(update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Retrieve the results\n",
    "We retrieve the results of the linkage. As we selected earlier, the result is a list of groups of records. Every record in such a group belongs to the same entity and consists of two values, the party id and the row index.\n",
    "\n",
    "The last 20 groups look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 287], [2, 293], [4, 277]],\n",
       " [[0, 2387], [1, 2386]],\n",
       " [[0, 264], [3, 252], [1, 272]],\n",
       " [[0, 2496], [4, 2498]],\n",
       " [[3, 147], [4, 147]],\n",
       " [[3, 815], [4, 812]],\n",
       " [[3, 1302], [4, 1343]],\n",
       " [[0, 1691], [3, 1674]],\n",
       " [[0, 3085], [3, 3117]],\n",
       " [[1, 2559], [4, 2545]],\n",
       " [[0, 574], [3, 576], [4, 554]],\n",
       " [[0, 424], [4, 387]],\n",
       " [[1, 1087], [2, 1140]],\n",
       " [[1, 468], [2, 489], [3, 482], [4, 469]],\n",
       " [[3, 2102], [4, 2115]],\n",
       " [[1, 981], [3, 1007]],\n",
       " [[0, 696], [3, 704]],\n",
       " [[0, 2475], [2, 2501], [1, 2485]],\n",
       " [[1, 1034], [2, 1090]],\n",
       " [[0, 2785], [4, 2797]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"{PREFIX}/projects/{project_id}/runs/{run_id}/result\",\n",
    "    headers={\n",
    "        \"Authorization\": result_token\n",
    "    }\n",
    ")\n",
    "groups = r.json()\n",
    "groups['groups'][-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sanity check, we print their records' corresponding PII:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['mackenzie', 'tremellen', '11-01-2947', 'maoe', 'melbourne', '79469.112', '']\n",
      "2 ['mackenzie', 'dremellen', '11-01-2937', 'mals', 'mceloburne', '70469.122', '07 5988 5208']\n",
      "4 ['macckenzie', 'tremellen', '', 'malr', 'melbovrne', '70469.122', '07 5988 5208']\n",
      "\n",
      "0 ['sophi', 'couljon', '12-03-1841', 'female', 'sydney', '80972.256', '04 3854 3784']\n",
      "1 ['sophie', 'coulson', '12-03-1941', 'female', 'sydney', '80972.356', '04 3854 3784']\n",
      "\n",
      "0 ['jasmine', 'clarke', '04-00-2009', 'maje', 'melb0urme', '99853.100', '02 1507 1520']\n",
      "3 ['jasmine', 'clarke', '04-09-2009', 'male', 'melbourne', '99853.200', '02 1507 1520']\n",
      "1 ['jasminr', 'klarle', '04-99-2009', 'male', 'melbourne', '99863.200', '02 1507 1520']\n",
      "\n",
      "0 ['zoel', 'ev', '06-09-1990', 'gemale', 'ysdnvvy', '183366.696', '02 5578 4520']\n",
      "4 ['joel', 'everett', '06-09-1990', 'female', 'sydney', '183366.696', '02 5578 4520']\n",
      "\n",
      "3 ['katelyn', 'matthets', '23-07-1977', '', 'melbourne', '118010.996', '07 9265 9238']\n",
      "4 ['kateyln', 'matth4ws', '23-07-1978', 'male', 'melbounre', '118010.996', '07 9265 9238']\n",
      "\n",
      "3 ['max', 'pontifex', '17-07-1930', 'male', 'melbourne', '42337.169', '04 8102 3785']\n",
      "4 ['max', 'pontjef', '17-07-1930', 'male', 'melbovrne', '', '04 9102 3785']\n",
      "\n",
      "3 ['talrna', 'seilo', '06-09-1953', 'maoe', '', '55815.962', '03 8568 8024']\n",
      "4 ['talezba', 'seib', '06-09-1953', 'male', '', '', '03 8567 8024']\n",
      "\n",
      "0 ['maddiaon', \"mel'ln\", '21-12-1945', 'male', 'melbouren', '', '02 1963 9316']\n",
      "3 ['madklidon', 'meJi7|', '21-12-1945', 'maie', 'melbourne', '98312.180', '02 1964 9316']\n",
      "\n",
      "0 ['holly', 'reih', '22-06-2009', 'msle', 'syconey', '131184.582', '']\n",
      "3 ['holly', 'reicl', '21-06-2009', 'male', 'sydey', '131184.582', '']\n",
      "\n",
      "1 ['jessica', 'peteahsen', '30-07-1940', 'malr', 'mel1>oume', '173806.400', '04 7005 4927']\n",
      "4 ['jes5ica', 'peter5en', '30-08-1040', 'male', 'melbourne', '173806.400', '04 7005 49q7']\n",
      "\n",
      "0 ['thomas', 'kositcin', '26-08-1939', 'male', 'melbourne', '43048.734', '07 4737 4471']\n",
      "3 ['tomas', 'kosutcin', '26-08-1939', 'msle', 'melbourne', '43048.735', '07 4737 4471']\n",
      "4 ['thornas', 'kos9tcin', '26-08-1939', 'male', 'melborune', '43948.734', '07 4737 4471']\n",
      "\n",
      "0 ['sofie', 'ny', '20-10-1933', 'fenale', '', '135685.300', '07 7905 6885']\n",
      "4 ['stofia', 'ny', '20-10-q933', 'female', 'sydnev', '135685.300', '07 7905 6885']\n",
      "\n",
      "1 ['sophie', 'mazx9ne', '25-03-2814', 'make', 'melbourne', '36878.525', '08 3679 2653']\n",
      "2 ['sofie', 'mazzone', '25-03-2924', 'mals', 'melbourne', '36878.526', '08 3678 2653']\n",
      "\n",
      "1 ['stephnaie', 'goldsworthy', '03-06-1958', '', 'canbrrra', '83372.67q', '02 4093 4044']\n",
      "2 ['sttepbanie', 'goldsworthy', '03-06-1958', 'mald', 'canbedra', '83372.772', '02 4093 4044']\n",
      "3 ['stefanie', 'goldsworthy', '03-06-1958', 'male', 'camberra', '83372.572', '']\n",
      "4 ['stefanie', 'go|dsworthy', '03-06-1958', '', 'cabr:erra', '83372.672', '02 4093 4044']\n",
      "\n",
      "3 ['antony', 'riean', '18-01-1908', 'male', 'canberra', '59633.334', '07 2734 8270']\n",
      "4 ['anthnoy', 'ryari', '18-01-1908', 'male', 'cajberra', '58633.434', '07 2734 8370']\n",
      "\n",
      "1 ['eiahn', 'greeti', '11-0e-1977', 'male', 'melbourne', '68538.966', '03 8798 1825']\n",
      "3 ['eirn', 'kreen', '11-04-1977', 'male', 'meluourne', '68548.95y', '03 8798 1825']\n",
      "\n",
      "0 ['aleesga', 'nkuyen', '14-06-1068', 'male', 'melbourrie', '122053.275', '02 6678 5223']\n",
      "3 ['aleeSa', 'nguyen', '14-o6-1968', 'male', 'mtelbournr', '122053.265', '02 6678 5223']\n",
      "\n",
      "0 ['benjamin', 'bishop', '25-11-1980', 'male', 'sydney', '95170.703', '04 3415 3977']\n",
      "2 [\"benzam'ln\", 'bish9p', '25-11-1980', 'msle', 'sydn3v', '95170.703', '04 3415 3977']\n",
      "1 ['bennie', 'bishop', '25-11-1980', 'mald', '', '95180.703', '04 3415 3977']\n",
      "\n",
      "1 [\"ke'Irx\", 'chappel', '19-05-1966', 'male', '', '138869.396', '']\n",
      "2 ['keira', 'chapepl', '19-05-1966', 'male', '', '148869.296', '']\n",
      "\n",
      "0 ['deagxan', 'zaffino', '22-01-1979', 'femame', 'sydne7', '99746.221', '04 1534 02e5']\n",
      "4 ['teagan', 'zaffino', '22-01-1979', 'female', 'sydney', '99746.221', '04 1534 0225']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(i):\n",
    "    dataset = []\n",
    "    with open(f\"data/dataset-{i}.csv\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # ignore header\n",
    "        for row in reader:\n",
    "            dataset.append(row[1:])\n",
    "    return dataset\n",
    "\n",
    "datasets = list(map(load_dataset, range(1, 6)))\n",
    "\n",
    "for group in itertools.islice(groups[\"groups\"][-20:], 20):\n",
    "    for (i, j) in group:\n",
    "        print(i, datasets[i][j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Despite the high amount of noise in the data, the entity service was able to produce a fairly accurate matching. However, Isabella George and Mia/Talia Galbraith are most likely not an actual match.\n",
    "\n",
    "We may be able to improve on this results by fine-tuning the hashing schema or by changing the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Delete the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "r = requests.delete(\n",
    "    f\"{PREFIX}/projects/{project_id}\",\n",
    "    headers={\n",
    "        \"Authorization\": result_token\n",
    "    }\n",
    ")\n",
    "print(r.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
