{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Service Mapping Output\n",
    "\n",
    "This tutorial demonstrates generating CLKs from PII, creating a new mapping on the entity service, and how to retrieve the results. The output type is a simple mapping.\n",
    "\n",
    "Note the sections are usually run on different companies - but for illustration all is carried out in this one file. The participants providing data are *Alice* and *Bob*, and the analyst acting the integration authority.\n",
    "\n",
    "### Who learns what?\n",
    "\n",
    "Alice and Bob will both generate and upload their CLKs.\n",
    "\n",
    "The analyst - who creates the linkage project - learns the `mapping`. The mapping lines up rows from Alice's data set to Bob's.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Data preparation\n",
    "  * Write CSV files with PII\n",
    "  * Create a Linkage Schema\n",
    "  \n",
    "* Create Linkage Project\n",
    "* Upload Data\n",
    "* Wait for completion\n",
    "* Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://linkage.data61.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mConnecting to Entity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mResponse: 200\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "{\"status\": \"ok\", \"rate\": 577166582, \"number_mappings\": 38}\n"
     ]
    }
   ],
   "source": [
    "!clkutil status --server \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Following the clkhash tutorial we will use a dataset from the `recordlinkage` library. We will just write both datasets out to temporary CSV files.\n",
    "\n",
    "If you are following along yourself you may have to adjust the file names in all the `!clkutil` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from recordlinkage.datasets import load_febrl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets written to /tmp/tmp7h__s2c4 and /tmp/tmpeujzxtzb\n"
     ]
    }
   ],
   "source": [
    "dfA, dfB = load_febrl4()\n",
    "\n",
    "a_csv = NamedTemporaryFile('w')\n",
    "a_clks = NamedTemporaryFile('w', suffix='.json')\n",
    "dfA.to_csv(a_csv)\n",
    "a_csv.seek(0)\n",
    "\n",
    "b_csv = NamedTemporaryFile('w')\n",
    "b_clks = NamedTemporaryFile('w', suffix='.json')\n",
    "dfB.to_csv(b_csv)\n",
    "b_csv.seek(0)\n",
    "\n",
    "dfA.head()\n",
    "print(\"Datasets written to {} and {}\".format(a_csv.name, b_csv.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Preparation\n",
    "\n",
    "The linkage schema must be agreed on by the two parties. The schema used in this tutorial is only the surname, first name and date of birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema written to /tmp/tmpxvtwrb72.yaml\n"
     ]
    }
   ],
   "source": [
    "column_metadata = [\n",
    "    'INDEX',\n",
    "    'NAME Surname',\n",
    "    'NAME First Name',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'DOB YYYY/MM/DD',\n",
    "    'INDEX'\n",
    "]\n",
    "\n",
    "schema = NamedTemporaryFile(\"wt\", suffix='.yaml')\n",
    "for col in column_metadata:\n",
    "    print('- identifier: \"{}\"'.format(col), file=schema)\n",
    "\n",
    "schema.seek(0)\n",
    "print(\"Schema written to\", schema.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Linkage Project\n",
    "\n",
    "The analyst carrying out the linkage starts by creating a linkage project of the desired output type with the Entity Service.\n",
    "In this case we are going to use a very high threshold which should prevent any false matches from showing up - however this is at the expense of missing possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials will be saved in /tmp/tmptbc8e7df\n",
      "\u001b[31mEntity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mServer Status: ok\u001b[0m\n",
      "\u001b[31mSchema: [{\"identifier\": \"INDEX\"}, {\"identifier\": \"NAME Surname\"}, {\"identifier\": \"NAME First Name\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"INDEX\"}, {\"identifier\": \"DOB YYYY/MM/DD\"}, {\"identifier\": \"INDEX\"}]\u001b[0m\n",
      "\u001b[31mType: mapping\u001b[0m\n",
      "\u001b[31mCreating new mapping\u001b[0m\n",
      "\u001b[31mMapping created\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resource_id': '8e993a2fd2036d6a4739d5369803f3c3fae8a13ee9dc5e19',\n",
       " 'result_token': 'd67af7f2e28ff6ff42e038ee15ec87a1a261e31e4ef73f29',\n",
       " 'update_tokens': ['6b99bda31f158912cba7995d8262b009ba2c78ee00fd5944',\n",
       "  '7a080b6a6839db66b40e56686b09f5a1cf423fad1fc50afa']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creds = NamedTemporaryFile('wt')\n",
    "print(\"Credentials will be saved in\", creds.name)\n",
    "\n",
    "!clkutil create --schema \"{schema.name}\" --output \"{creds.name}\" --type \"mapping\" --server \"{url}\" --threshold 0.97\n",
    "creds.seek(0)\n",
    "\n",
    "import json\n",
    "with open(creds.name, 'r') as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "mid = credentials['resource_id']\n",
    "credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the analyst will need to pass on the `resource_id` (the id of the linkage project) and one of the two `update_tokens` to each data provider.\n",
    "\n",
    "## Hash and Upload\n",
    "\n",
    "At the moment both data providers have *raw* personally identiy information. We first have to generate CLKs from the raw entity information. Please see [clkhash](https://clkhash.readthedocs.io/) documentation for further details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00K/5.00K [00:01<00:00, 1.26Kclk/s, mean=495, std=46.1]\n",
      "\u001b[31mCLK data written to /tmp/tmpqoj38cx5.json\u001b[0m\n",
      "generating CLKs: 100%|█| 5.00K/5.00K [00:01<00:00, 1.26Kclk/s, mean=489, std=52.8]\n",
      "\u001b[31mCLK data written to /tmp/tmp3uawaaok.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil hash --schema \"{schema.name}\" \"{a_csv.name}\" horse staple \"{a_clks.name}\"\n",
    "!clkutil hash --schema \"{schema.name}\" \"{b_csv.name}\" horse staple \"{b_clks.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two clients can upload their data providing the appropriate *upload tokens*. As with all commands in `clkhash` we can output help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: clkutil upload [OPTIONS] INPUT\n",
      "\n",
      "  Upload CLK data to entity matching server.\n",
      "\n",
      "  Given a json file containing hashed clk data as INPUT, upload to the\n",
      "  entity resolution service.\n",
      "\n",
      "  Use \"-\" to read from stdin.\n",
      "\n",
      "Options:\n",
      "  --mapping TEXT         Server identifier of the mapping\n",
      "  --apikey TEXT          Authentication API key for the server.\n",
      "  --server TEXT          Server address including protocol\n",
      "  -o, --output FILENAME\n",
      "  -v, --verbose          Script is more talkative\n",
      "  --help                 Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!clkutil upload --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice uploads her data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mUploading CLK data from /tmp/tmpqoj38cx5.json\u001b[0m\n",
      "\u001b[31mTo Entity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mMapping ID: 8e993a2fd2036d6a4739d5369803f3c3fae8a13ee9dc5e19\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "\u001b[31mUploading CLK data to the server\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with NamedTemporaryFile('wt') as f:\n",
    "    !clkutil upload \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][0]}\" \\\n",
    "        --server \"{url}\" \\\n",
    "        --output \"{f.name}\" \\\n",
    "        \"{a_clks.name}\"\n",
    "    res = json.load(open(f.name))\n",
    "    alice_receipt_token = res['receipt-token']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every upload gets a receipt token. In some operating modes this receipt is required to access parts of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bob uploads his data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mUploading CLK data from /tmp/tmp3uawaaok.json\u001b[0m\n",
      "\u001b[31mTo Entity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mMapping ID: 8e993a2fd2036d6a4739d5369803f3c3fae8a13ee9dc5e19\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "\u001b[31mUploading CLK data to the server\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with NamedTemporaryFile('wt') as f:\n",
    "    !clkutil upload \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][1]}\" \\\n",
    "        --server \"{url}\" \\\n",
    "        --output \"{f.name}\" \\\n",
    "        \"{b_clks.name}\"\n",
    "    bob_receipt_token = json.load(open(f.name))['receipt-token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now after some delay (depending on the size) we can fetch the mask.\n",
    "This can be done with clkutil:\n",
    "\n",
    "    !clkutil results \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['result_token']}\" --output results.txt\n",
    "        \n",
    "However for this tutorial we are going to use the Python `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.039098}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.185084}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.340995}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.505254}\n"
     ]
    }
   ],
   "source": [
    "result = requests.get('{}/api/v1/mappings/{}'.format(url, mid), headers={'Authorization': credentials['result_token']})\n",
    "while result.status_code != 200:\n",
    "    print(result.json())\n",
    "    result = requests.get('{}/api/v1/mappings/{}'.format(url, mid), headers={'Authorization': credentials['result_token']})\n",
    "    time.sleep(0.5)\n",
    "else:\n",
    "    results = result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Entity Service are mapping str -> str, to make it easier to work with we will create a dict of int -> int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {int(k): int(results['mapping'][k]) for k in results['mapping']}\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping links rows in Alice's data set to rows in Bob's - let's check that now. For the first few lines of Alice's data let's see if there a corrosponding entity has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2750\n",
      "2 4656\n",
      "3 4119\n",
      "4 3306\n",
      "5 2305\n",
      "6 3944\n",
      "7 992\n",
      "8 4612\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i in mapping:\n",
    "        print(i, mapping[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the raw data from Alice and Bob to see if the names match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(a_csv.name, 'r') as f:\n",
    "    alice_raw = f.readlines()\n",
    "\n",
    "with open(b_csv.name, 'r') as f:\n",
    "    bob_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec_id,given_name,surname,street_number,address_1,address_2,suburb,postcode,state,date_of_birth,soc_sec_id\\n',\n",
       " 'rec-1070-org,michaela,neumann,8,stanley street,miami,winston hills,4223,nsw,19151111,5304218\\n',\n",
       " 'rec-1016-org,courtney,painter,12,pinkerton circuit,bega flats,richlands,4560,vic,19161214,4066625\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the file includes a header row so we will add an offset of `+1` to our file lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2750 rec-1016-org rec-1016-dup-0 Courtney Painter Courtney Painter\n",
      "2 4656 rec-4405-org rec-4405-dup-0 Charles Green Charles Green\n",
      "3 4119 rec-1288-org rec-1288-dup-0 Vanessa Parr Vanessa Parr\n",
      "4 3306 rec-3585-org rec-3585-dup-0 Mikayla Malloney Mikayla Malloney\n",
      "5 2305 rec-298-org rec-298-dup-0 Blake Howie Blake Howie\n",
      "6 3944 rec-1985-org rec-1985-dup-0  Lund  Lund\n",
      "7 992 rec-2404-org rec-2404-dup-0 Blakeston Broadby Blakeston Broadby\n",
      "8 4612 rec-1473-org rec-1473-dup-0  Leslie  Leslie\n"
     ]
    }
   ],
   "source": [
    "for alice_i in range(10):\n",
    "    if alice_i in mapping:\n",
    "        bob_i = mapping[alice_i]\n",
    "        alice_row = alice_raw[alice_i+1].split(',')\n",
    "        bob_row = bob_raw[bob_i+1].split(',')\n",
    "        print(alice_i, bob_i, alice_row[0], bob_row[0], ' '.join(alice_row[1:3]).title(), ' '.join(bob_row[1:3]).title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "To compute how well the matching went we will use the original index as our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_correct = 0\n",
    "for alice_i in mapping:\n",
    "    bob_i = mapping[alice_i]\n",
    "    alice_row = alice_raw[alice_i+1].split(',')\n",
    "    bob_row = bob_raw[bob_i+1].split(',')\n",
    "    \n",
    "    if alice_row[0].split('-')[1] == bob_row[0].split('-')[1]:\n",
    "        number_correct += 1\n",
    "    else:\n",
    "        print(\"A false match!\", alice_row[0], bob_row[0], alice_row[-1], bob_row[-1], ' '.join(alice_row[1:3]).title(), ' '.join(bob_row[1:3]).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273\n"
     ]
    }
   ],
   "source": [
    "print(number_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
