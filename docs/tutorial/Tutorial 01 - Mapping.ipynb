{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Service Mapping Output\n",
    "\n",
    "This tutorial demonstrates generating CLKs from PII, creating a new mapping on the entity service, and how to retrieve the results. The output type is a simple mapping.\n",
    "\n",
    "Note the sections are usually run on different companies - but for illustration all is carried out in this one file. The participants providing data are *Alice* and *Bob*, and the analyst acting the integration authority.\n",
    "\n",
    "### Who learns what?\n",
    "\n",
    "Alice and Bob will both generate and upload their CLKs.\n",
    "\n",
    "The analyst - who creates the linkage project - learns the `mapping`. The mapping lines up rows from Alice's data set to Bob's.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Data preparation\n",
    "  * Write CSV files with PII\n",
    "  * Create a Linkage Schema\n",
    "  \n",
    "* Create Linkage Project\n",
    "* Upload Data\n",
    "* Wait for completion\n",
    "* Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://testing.es.data61.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"rate\": 20973, \"project_count\": 190, \"status\": \"ok\"}\n"
     ]
    }
   ],
   "source": [
    "!clkutil status --server \"{url}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Following the clkhash tutorial we will use a dataset from the `recordlinkage` library. We will just write both datasets out to temporary CSV files.\n",
    "\n",
    "If you are following along yourself you may have to adjust the file names in all the `!clkutil` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "from recordlinkage.datasets import load_febrl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets written to /tmp/tmpf1bzhc9o and /tmp/tmp4g9wibnm\n"
     ]
    }
   ],
   "source": [
    "dfA, dfB = load_febrl4()\n",
    "\n",
    "a_csv = NamedTemporaryFile('w')\n",
    "a_clks = NamedTemporaryFile('w', suffix='.json')\n",
    "dfA.to_csv(a_csv)\n",
    "a_csv.seek(0)\n",
    "\n",
    "b_csv = NamedTemporaryFile('w')\n",
    "b_clks = NamedTemporaryFile('w', suffix='.json')\n",
    "dfB.to_csv(b_csv)\n",
    "b_csv.seek(0)\n",
    "\n",
    "dfA.head()\n",
    "print(\"Datasets written to {} and {}\".format(a_csv.name, b_csv.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Preparation\n",
    "\n",
    "The linkage schema must be agreed on by the two parties. The schema used in this tutorial is only the surname, first name and date of birth. All columns marked as `INDEX` will be ignored by clkhash when creating CLKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema written to /tmp/tmp78du25xm.yaml\n"
     ]
    }
   ],
   "source": [
    "column_metadata = [\n",
    "    'INDEX',\n",
    "    'NAME Surname',\n",
    "    'NAME First Name',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'INDEX',\n",
    "    'DOB YYYY/MM/DD',\n",
    "    'INDEX'\n",
    "]\n",
    "\n",
    "schema = NamedTemporaryFile(\"wt\", suffix='.yaml')\n",
    "# TODO WRITE NEW SCHEMA HERE\n",
    "schema.seek(0)\n",
    "print(\"Schema written to\", schema.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Linkage Project\n",
    "\n",
    "The analyst carrying out the linkage starts by creating a linkage project of the desired output type with the Entity Service.\n",
    "In this case we are going to use a very high threshold which should prevent any false matches from showing up - however this is at the expense of missing possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credentials will be saved in /tmp/tmpplwtc0wr\n",
      "\u001b[31mEntity Matching Server: https://testing.es.data61.xyz\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/clkhash/__main__.py\", line 4, in <module>\n",
      "    cli()\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/click/core.py\", line 722, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/click/core.py\", line 697, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/click/core.py\", line 895, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/click/core.py\", line 535, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/root/testenv/lib/python3.5/site-packages/clkhash/cli.py\", line 153, in create_project\n",
      "    schema_json = json.load(schema)\n",
      "  File \"/usr/lib/python3.5/json/__init__.py\", line 268, in load\n",
      "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "  File \"/usr/lib/python3.5/json/__init__.py\", line 319, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/lib/python3.5/json/decoder.py\", line 339, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/lib/python3.5/json/decoder.py\", line 357, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creds = NamedTemporaryFile('wt')\n",
    "print(\"Credentials will be saved in\", creds.name)\n",
    "\n",
    "!python -m clkhash create-project -v --schema \"{schema.name}\" --output \"{creds.name}\" --type \"mapping\" --server \"{url}\"\n",
    "creds.seek(0)\n",
    "\n",
    "import json\n",
    "with open(creds.name, 'r') as f:\n",
    "    print(f.read())\n",
    "    #credentials = json.load(f)\n",
    "\n",
    "#pid = credentials['project_id']\n",
    "#credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the analyst will need to pass on the `resource_id` (the id of the linkage project) and one of the two `update_tokens` to each data provider.\n",
    "\n",
    "## Hash and Upload\n",
    "\n",
    "At the moment both data providers have *raw* personally identiy information. We first have to generate CLKs from the raw entity information. Please see [clkhash](https://clkhash.readthedocs.io/) documentation for further details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 5.00K/5.00K [00:01<00:00, 1.26Kclk/s, mean=495, std=46.1]\n",
      "\u001b[31mCLK data written to /tmp/tmpqoj38cx5.json\u001b[0m\n",
      "generating CLKs: 100%|█| 5.00K/5.00K [00:01<00:00, 1.26Kclk/s, mean=489, std=52.8]\n",
      "\u001b[31mCLK data written to /tmp/tmp3uawaaok.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil hash --schema \"{schema.name}\" \"{a_csv.name}\" horse staple \"{a_clks.name}\"\n",
    "!clkutil hash --schema \"{schema.name}\" \"{b_csv.name}\" horse staple \"{b_clks.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two clients can upload their data providing the appropriate *upload tokens*. As with all commands in `clkhash` we can output help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: clkutil upload [OPTIONS] INPUT\n",
      "\n",
      "  Upload CLK data to entity matching server.\n",
      "\n",
      "  Given a json file containing hashed clk data as INPUT, upload to the\n",
      "  entity resolution service.\n",
      "\n",
      "  Use \"-\" to read from stdin.\n",
      "\n",
      "Options:\n",
      "  --mapping TEXT         Server identifier of the mapping\n",
      "  --apikey TEXT          Authentication API key for the server.\n",
      "  --server TEXT          Server address including protocol\n",
      "  -o, --output FILENAME\n",
      "  -v, --verbose          Script is more talkative\n",
      "  --help                 Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!clkutil upload --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice uploads her data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mUploading CLK data from /tmp/tmpqoj38cx5.json\u001b[0m\n",
      "\u001b[31mTo Entity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mMapping ID: 8e993a2fd2036d6a4739d5369803f3c3fae8a13ee9dc5e19\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "\u001b[31mUploading CLK data to the server\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with NamedTemporaryFile('wt') as f:\n",
    "    !clkutil upload \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][0]}\" \\\n",
    "        --server \"{url}\" \\\n",
    "        --output \"{f.name}\" \\\n",
    "        \"{a_clks.name}\"\n",
    "    res = json.load(open(f.name))\n",
    "    alice_receipt_token = res['receipt-token']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every upload gets a receipt token. In some operating modes this receipt is required to access parts of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bob uploads his data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mUploading CLK data from /tmp/tmp3uawaaok.json\u001b[0m\n",
      "\u001b[31mTo Entity Matching Server: https://linkage.data61.xyz\u001b[0m\n",
      "\u001b[31mMapping ID: 8e993a2fd2036d6a4739d5369803f3c3fae8a13ee9dc5e19\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "\u001b[31mUploading CLK data to the server\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with NamedTemporaryFile('wt') as f:\n",
    "    !clkutil upload \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['update_tokens'][1]}\" \\\n",
    "        --server \"{url}\" \\\n",
    "        --output \"{f.name}\" \\\n",
    "        \"{b_clks.name}\"\n",
    "    bob_receipt_token = json.load(open(f.name))['receipt-token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now after some delay (depending on the size) we can fetch the mask.\n",
    "This can be done with clkutil:\n",
    "\n",
    "    !clkutil results \\\n",
    "        --mapping=\"{credentials['resource_id']}\" \\\n",
    "        --apikey=\"{credentials['result_token']}\" --output results.txt\n",
    "        \n",
    "However for this tutorial we are going to use the Python `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.0}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.039098}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.185084}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.340995}\n",
      "{'current': '0', 'progress': 0.0, 'message': \"Mapping isn't ready.\", 'total': '25000000', 'elapsed': 0.505254}\n"
     ]
    }
   ],
   "source": [
    "result = requests.get('{}/api/v1/mappings/{}'.format(url, mid), headers={'Authorization': credentials['result_token']})\n",
    "while result.status_code != 200:\n",
    "    print(result.json())\n",
    "    result = requests.get('{}/api/v1/mappings/{}'.format(url, mid), headers={'Authorization': credentials['result_token']})\n",
    "    time.sleep(0.5)\n",
    "else:\n",
    "    results = result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Entity Service are mapping str -> str, to make it easier to work with we will create a dict of int -> int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {int(k): int(results['mapping'][k]) for k in results['mapping']}\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping links rows in Alice's data set to rows in Bob's - let's check that now. For the first few lines of Alice's data let's see if there a corrosponding entity has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2750\n",
      "2 4656\n",
      "3 4119\n",
      "4 3306\n",
      "5 2305\n",
      "6 3944\n",
      "7 992\n",
      "8 4612\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i in mapping:\n",
    "        print(i, mapping[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the raw data from Alice and Bob to see if the names match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(a_csv.name, 'r') as f:\n",
    "    alice_raw = f.readlines()\n",
    "\n",
    "with open(b_csv.name, 'r') as f:\n",
    "    bob_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec_id,given_name,surname,street_number,address_1,address_2,suburb,postcode,state,date_of_birth,soc_sec_id\\n',\n",
       " 'rec-1070-org,michaela,neumann,8,stanley street,miami,winston hills,4223,nsw,19151111,5304218\\n',\n",
       " 'rec-1016-org,courtney,painter,12,pinkerton circuit,bega flats,richlands,4560,vic,19161214,4066625\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the file includes a header row so we will add an offset of `+1` to our file lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2750 rec-1016-org rec-1016-dup-0 Courtney Painter Courtney Painter\n",
      "2 4656 rec-4405-org rec-4405-dup-0 Charles Green Charles Green\n",
      "3 4119 rec-1288-org rec-1288-dup-0 Vanessa Parr Vanessa Parr\n",
      "4 3306 rec-3585-org rec-3585-dup-0 Mikayla Malloney Mikayla Malloney\n",
      "5 2305 rec-298-org rec-298-dup-0 Blake Howie Blake Howie\n",
      "6 3944 rec-1985-org rec-1985-dup-0  Lund  Lund\n",
      "7 992 rec-2404-org rec-2404-dup-0 Blakeston Broadby Blakeston Broadby\n",
      "8 4612 rec-1473-org rec-1473-dup-0  Leslie  Leslie\n"
     ]
    }
   ],
   "source": [
    "for alice_i in range(10):\n",
    "    if alice_i in mapping:\n",
    "        bob_i = mapping[alice_i]\n",
    "        alice_row = alice_raw[alice_i+1].split(',')\n",
    "        bob_row = bob_raw[bob_i+1].split(',')\n",
    "        print(alice_i, bob_i, alice_row[0], bob_row[0], ' '.join(alice_row[1:3]).title(), ' '.join(bob_row[1:3]).title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "To compute how well the matching went we will use the original index as our reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_correct = 0\n",
    "for alice_i in mapping:\n",
    "    bob_i = mapping[alice_i]\n",
    "    alice_row = alice_raw[alice_i+1].split(',')\n",
    "    bob_row = bob_raw[bob_i+1].split(',')\n",
    "    \n",
    "    if alice_row[0].split('-')[1] == bob_row[0].split('-')[1]:\n",
    "        number_correct += 1\n",
    "    else:\n",
    "        print(\"A false match!\", alice_row[0], bob_row[0], alice_row[-1], bob_row[-1], ' '.join(alice_row[1:3]).title(), ' '.join(bob_row[1:3]).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273\n"
     ]
    }
   ],
   "source": [
    "print(number_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
