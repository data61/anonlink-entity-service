{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Service Similarity Score Output\n",
    "\n",
    "This example shows how to retrieve the similarity scores from the Entity Service. Note if you are running this notebook it will save multiple files into your working directory.\n",
    "\n",
    "First we create a new mapping with the type set to `similarity_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntity Matching Server: https://es.data61.xyz\u001b[0m\n",
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mServer Status: ok\u001b[0m\n",
      "\u001b[31mSchema: NOT PROVIDED\u001b[0m\n",
      "\u001b[31mType: similarity_scores\u001b[0m\n",
      "\u001b[31mCreating new mapping\u001b[0m\n",
      "\u001b[31mMapping created\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!clkutil create \\\n",
    "    --type similarity_scores \\\n",
    "    --threshold 0.95 \\\n",
    "    --output credentials.json \\\n",
    "    --server https://es.data61.xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the credentials saved by the command line tool into a Python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "with open('credentials.json','r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some entity information to match. For testing purposes the `clkhash` tool can generate fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!clkutil generate 2000 raw_pii_2k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the fake PII data into somewhat overlapping sets.\n",
    "Alice will have 1500 enties, bob will have 1000, and 500 entities overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -n 1 raw_pii_2k.csv > alice.txt\n",
    "!tail -n 1500 raw_pii_2k.csv >> alice.txt\n",
    "!head -n 1000 raw_pii_2k.csv > bob.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated data is a very simple fake PII: `ID, Name, YOB, Gender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997,Vivian Modi,1933/03/31,F\n",
      "998,Latonia Shumpert,1958/01/15,F\n"
     ]
    }
   ],
   "source": [
    "!tail -n 2 bob.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated *raw* identiy information which will have to be hashed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CLKs: 100%|█| 1.50K/1.50K [00:00<00:00, 1.11Kclk/s, mean=521, std=36.4]\n",
      "\u001b[31mCLK data written to alice-hashed.json\u001b[0m\n",
      "generating CLKs: 100%|███| 999/999 [00:00<00:00, 4.86Kclk/s, mean=521, std=36.4]\n",
      "\u001b[31mCLK data written to bob-hashed.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Hash the data using the secret keys that the linkage authority doesn't know\n",
    "!clkutil hash alice.txt horse staple  alice-hashed.json\n",
    "!clkutil hash bob.txt horse staple  bob-hashed.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_data(mapping, apikey, server, data):\n",
    "    response = requests.put(\n",
    "        '{}/api/v1/mappings/{}'.format(server, mapping),\n",
    "        data=data,\n",
    "        headers={\n",
    "            \"Authorization\": apikey,\n",
    "            'content-type': 'application/json'\n",
    "        }\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_upload_response = upload_data(\n",
    "    mapping=credentials['resource_id'], \n",
    "    apikey=credentials['update_tokens'][0], \n",
    "    server=\"https://es.data61.xyz\",\n",
    "    data=open('alice-hashed.json','r')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bob_upload_response = upload_data(\n",
    "    mapping=credentials['resource_id'], \n",
    "    apikey=credentials['update_tokens'][1], \n",
    "    server=\"https://es.data61.xyz\",\n",
    "    data=open('bob-hashed.json','r')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every upload gets a receipt token. In some operating modes this receipt is required to access the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mid = credentials['resource_id']\n",
    "alice_receipt_token = alice_upload_response['receipt-token']\n",
    "bob_receipt_token = bob_upload_response['receipt-token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mChecking server status\u001b[0m\n",
      "\u001b[31mStatus: ok\u001b[0m\n",
      "\u001b[31mResponse code: 200\u001b[0m\n",
      "\u001b[31mReceived result\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Now after some delay (depending on the size) we can fetch the resulting sparse matrix\n",
    "!clkutil results \\\n",
    "    --mapping=\"{credentials['resource_id']}\" \\\n",
    "    --server https://es.data61.xyz \\\n",
    "    --apikey=\"{credentials['result_token']}\" --output results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results.txt','r') as f:\n",
    "    sparse_scores = json.load(f)['similarity_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0 1.0\n",
      "501 1 1.0\n",
      "502 2 1.0\n",
      "503 3 1.0\n",
      "504 4 1.0\n",
      "505 5 1.0\n",
      "506 6 1.0\n",
      "507 7 1.0\n",
      "508 8 1.0\n",
      "509 9 1.0\n",
      "510 10 1.0\n",
      "511 11 1.0\n",
      "512 12 1.0\n",
      "513 13 1.0\n",
      "514 14 1.0\n",
      "515 15 1.0\n",
      "516 16 1.0\n",
      "517 17 1.0\n",
      "518 18 1.0\n",
      "519 19 1.0\n",
      "520 20 1.0\n",
      "521 21 1.0\n"
     ]
    }
   ],
   "source": [
    "for i, (index_a, index_b, dice_score) in enumerate(sparse_scores):\n",
    "    print(index_a, index_b, dice_score)\n",
    "    \n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the data hasn't been pertubed so the results are all 1.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
